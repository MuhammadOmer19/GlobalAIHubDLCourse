# -*- coding: utf-8 -*-
"""Homework_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ObCqfgL7Mpyyss8AsY6acx5HNCm4JxkF

### Importing libraries
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import mnist
from keras.models import Sequential,model_from_json
from keras.layers import Dense
from keras.optimizers import RMSprop
import pylab as plt

"""#### Keras is the deep learning library that helps you to code Deep Neural Networks with fewer lines of code

### Import data
"""

batch_size = 128
num_classes = 10
epochs = 2

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# Normalize to 0 to 1 range
x_train /= 255
x_test /= 255

print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

"""# Part 1: Analysis of Activation Functions

### Try other Activation Functions
Analyze the accuracy and training time
#### Example Syntax 
* model.add(Dense(8, activation='tanh'))
* model.add(Dense(8, activation='linear'))
* model.add(Dense(8, activation='relu'))
* model.add(Dense(8, activation='hard_sigmoid'))

### Sigmoid Activation Function
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# model = Sequential()
# model.add(Dense(64, activation='sigmoid', input_shape=(784,)))
# model.add(Dense(64, activation='sigmoid'))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='categorical_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=10,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])

"""### Relu Activation Function"""

# Commented out IPython magic to ensure Python compatibility.
# # Write your code here. Use the same architecture as above. 
# %%time
# 
# model = Sequential()
# model.add(Dense(64, activation='relu', input_shape=(784,)))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='categorical_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=10,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])
#

"""### Write your findings about activations here?

1.   Finding 1
2.   Finding 2

1.   Finding 1: The Sigmoid activation function yields a lower accuracy than the RELU activation.
2.   Finding2: The RELU activation function yields a higher accuracy than the Sigmoid activation.

Both take the same amount of computational time.

# Part 2: Design Choices: Nodes
#### Analyze the parameter size, accuracy and training time

### Design a model with small Number of Nodes. For Example 8
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# first_layer_size = 8
# 
# model = Sequential()
# model.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
# model.add(Dense(32, activation='sigmoid'))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='categorical_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=epochs,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])
# 
# w = []
# for layer in model.layers:
#     weights = layer.get_weights()
#     w.append(weights)
# 
# layer1 = np.array(w[0][0])
# print("Shape of First Layer",layer1.shape)
# print("Visualization of First Layer")
# 
# import matplotlib.pyplot as plt
# fig=plt.figure(figsize=(16, 16))
# columns = 8
# rows = int(first_layer_size/8)
# for i in range(1, columns*rows +1):
#     fig.add_subplot(rows, columns, i)
#     plt.imshow(layer1[:,i-1].reshape(28,28),cmap='gray')
# plt.show()

"""### Design a model with large Number of Nodes. For example 128"""

# Commented out IPython magic to ensure Python compatibility.
# # Write your code here 
# %%time
# 
# first_layer_size = 128
# 
# model = Sequential()
# model.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
# model.add(Dense(32, activation='sigmoid'))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='categorical_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=epochs,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])
# 
# w = []
# for layer in model.layers:
#     weights = layer.get_weights()
#     w.append(weights)
# 
# layer1 = np.array(w[0][0])
# print("Shape of First Layer",layer1.shape)
# print("Visualization of First Layer")
# 
# import matplotlib.pyplot as plt
# fig=plt.figure(figsize=(16, 16))
# columns = 8
# rows = int(first_layer_size/8)
# for i in range(1, columns*rows +1):
#     fig.add_subplot(rows, columns, i)
#     plt.imshow(layer1[:,i-1].reshape(28,28),cmap='gray')
# plt.show()
# # Use the same layer design from the above cell. Play with the parameter "first_layer_size" above. 
# # It is preferred to have first_layer_size as multiplication of 8 (for visualizing the weights properly)

"""### Write your findings about influence of nodes here?

1.   Finding 1
2.   Finding 2

1.   Finding 1: Model with lower number of nodes takes less computational time, however it also yields lower accuracy, due to the low complexity of the NN.
2.   Finding 2: Model with higher number of nodes takes more computational time, and it yields higher accuracy, as the NN becomes complex.

# Part 3: Design Choices: Layers
#### Analyze the parameter size, accuracy and training time

### Design a model with small number of layers. For example 1 hidden layer
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# model = Sequential()
# model.add(Dense(32, activation='relu', input_shape=(784,)))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='categorical_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=10,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# model = Sequential()
# model.add(Dense(32, activation='tanh', input_shape=(784,)))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='categorical_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=10,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# model = Sequential()
# model.add(Dense(32, activation='tanh', input_shape=(784,)))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='binary_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=10,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])

"""### Lower number of Layers. For example 4 hidden layers"""

# Commented out IPython magic to ensure Python compatibility.
# # Write your code here 
# %%time 
# 
# model = Sequential()
# model.add(Dense(32, activation='relu', input_shape=(784,)))
# model.add(Dense(32, activation='relu', input_shape=(784,)))
# model.add(Dense(32, activation='relu', input_shape=(784,)))
# model.add(Dense(32, activation='relu', input_shape=(784,)))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='categorical_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=10,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])
# # Use the same number of units from the above cell

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# 
# model = Sequential()
# model.add(Dense(32, activation='relu', input_shape=(784,)))
# model.add(Dense(32, activation='tanh', input_shape=(784,)))
# model.add(Dense(32, activation='sigmoid', input_shape=(784,)))
# model.add(Dense(32, activation='relu', input_shape=(784,)))
# model.add(Dense(num_classes, activation='softmax'))
# 
# model.summary()
# model.compile(loss='categorical_crossentropy',
#               optimizer=RMSprop(),
#               metrics=['accuracy'])
# 
# history = model.fit(x_train, y_train,
#                     batch_size=batch_size,
#                     epochs=10,
#                     verbose=1,
#                     validation_data=(x_test, y_test))
# score = model.evaluate(x_test, y_test, verbose=0)
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])

"""### Write your findings about influence of layers here?

1.   Finding 1
2.   Finding 2

1.   Finding 1: With lesser hidden layers, the accuracy was high and time was lesser.
2.   Finding 2: Adding more hidden layers actually decreased the accuracy, which was surprising, while it took more time.

Furthermore, upon investigation, it was observed that using tanh activation instead of RELU took more time while it yielded lower accuracy. 

In addition to that, using tanh as activation, and changing the loss function from categorical to binary crossentrophy caused the same effect, with decreased accuracy and increased time. 

Finally, with multiple layers, if all the layers had RELU as the activation function, it took lesser time however it also yielded lower accuracy, compared to the scenario where the layers were mixed up with RELU, tanh and sigmoid.
"""